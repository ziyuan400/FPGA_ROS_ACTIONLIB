\chapter{Related Work}
\label{sec:related_work}

The preemptable scheduler problem is a traditional \gls{rtos}, and many \gls{rtos}s have solved this problem in software and published their solutions in their open source code and documentation. For example, μC/OS-III is a preemptive multitasking kernel\cite{microCOS}, so μC/OS-III always runs the most important prepare-to-run tasks. μC/OS-III is a scalable, ROM-capable, preemptive real-time kernel that can manage an unlimited number of tasks. μC/OS-III is a third-generation kernel that provides all the services expected of a modern real-time kernel, such as resource management, synchronization, inter-task communication, and more. However, μC/OS-III offers many unique features not found in other real-time kernels, such as the ability to perform performance measurements at runtime, to signal or message tasks directly, to implement suspend on multiple kernel objects, and more of.

The need for faster response times to external stimuli for fast processing has led to intensive research into processor and \gls{rtos} architectures\cite{Stallings2008}. In this case, most researchers in the field have concluded that certain components (or even the entire HW-RTOS must be embedded in hardware, as it enables increased parallel processing of information, thereby reducing the responsiveness of embedded systems era.

Yi Tang et al. A task queue-based hardware scheduler is introduced for \gls{fpga}-based embedded real-time systems\cite{TangYi2015}. A hardware scheduler is developed to improve the real-time performance of soft-core processor-based computing systems. Hardware schedulers typically accelerate system performance at the expense of increased hardware resources, inflexibility, and integration difficulties. However, the reprogrammability of \gls{fpga}-based systems eliminates the problems of inflexibility and integration difficulties. SR-PQ implements a series of shift register blocks that self-order according to their priorities\cite{Saez1999}. In addition to priority, the queue block also stores an address field, which is the task identifier (TID) in the task queue. Each node has two parallel entries. The new ingress bus is used to input queue data, while the other input issues control commands (read and write). In addition to this, each block is connected to adjacent blocks. The main disadvantages of hardware schedulers are increased hardware usage and algorithm inflexibility. However, small embedded systems have a limited number of tasks and predefined functions. The hardware scheduler on an \gls{fpga} system can be packaged as a customizable IP core. This makes it easy to use and also allows for customization on an application-by-application basis. For example, instead of choosing a fixed number of tasks in the scheduler to support, cores can be tailored to the number of tasks in the target system. This makes the hardware scheduler a more attractive option.

Ionel Zagan et al. introduced Hardware \gls{rtos}, a custom scheduler implementation based on multiple pipeline registers and MIPS32 architecture\cite{Zagan2020}. Task context switching operations, inter-task synchronization and communication mechanisms, and jitter when dealing with aperiodic events are key factors in implementing a real-time operating system (\gls{rtos}). In practice and literature, several solutions can be identified to improve the responsiveness and performance of real-time systems\cite{dodiu2012custom}. Software implementations of \gls{rtos}-specific functions can introduce significant delays that can adversely affect deadlines required by some applications. Their work presents the original implementation of a dedicated processor based on multiple pipeline registers, as well as hardware support for a dynamic scheduler that performs single event management, provides access to architecturally shared resources, prioritizes and executes multiple expected events for the same task\cite{Zagan2019}. Their work also presents a method for assigning interrupts to tasks. Through dedicated instructions, the integrated hardware scheduler achieves task synchronization with multiple priority events, thereby ensuring the efficient operation of the processor in a real-time control environment.

Davide Conficconi et al. designed a framework for customizable \gls{fpga}-based image registration accelerators\cite{Conficconi2021}. The scheduler design is generalized for input fetching of the particular scenario to fit many \gls{fpga} devices. At the very beginning of the execution, the architecture acquires two input images (reference and floating). This step depends on the physical memory ports of the device and the available bandwidth. Since not all \gls{fpga}-based devices offer multiple memory ports, in order to be as versatile as possible, we consider the case where a single memory port is multiplexed (in the case of having multiple ports, the gas pedal can be replicated depending on the number of ports). However, this situation may impair the performance of the accelerator, especially in the current memory-constrained design. Therefore, it is of utmost importance to design the gas pedal correctly according to the bit width of the memory ports and the memory bandwidth. A solution to alleviate this problem could be to prefetch one or two images on local memory. This is particularly applicable to algorithms like image registration. In fact, in order to register two images, the reference image does not change throughout the optimization process, while the floating image keeps changing. Therefore, if the target \gls{fpga} has enough on-chip memory and we apply this feature, our architecture first prefetches the reference image and then reads the floating image as a stream; otherwise, it reads both images simultaneously.

Kaiyuan Guo et al. investigate state-of-the-art CNN models and CNN-based applications and introduced Angel-Eye\cite{GuoKaiyuan2018}, a complete design flow for mapping CNN onto embedded \gls{fpga}. For embedded platforms, CNN-based solutions are too complex to be applied. Various dedicated hardware designs on \gls{fpga}s have been carried out to accelerate CNNs, while few of them explore the whole design flow for both fast deployment and high power efficiency.  Requirements on memory, computation and the flexibility of the system are summarized for mapping CNN on embedded \gls{fpga}s. Based on these requirements, we propose Angel-Eye, a programmable and flexible CNN accelerator architecture, together with data quantization strategy and compilation tool. Data quantization strategy helps reduce the bit-width down to 8-bit with negligible accuracy loss. The compilation tool maps a certain CNN model efficiently onto hardware. Controller part receives, decodes and issues instructions to the other three parts. Controller monitors the work state of each part and checks if the current instruction to this part can be issued. Thus, the host can send the generated instructions to controller through a simple FIFO interface and wait for the work to finish by checking the state registers in controller. This reduces the scheduling overhead for the host at run-time. Other tasks can be done with the host CPU when CNN is running. Parallel execution of instructions may cause data hazard. In hardware, an instruction is executed if: 1) the corresponding hardware is free and 2) the instructions it depends on have finished. Condition 1 is maintained by LOAD Ins FIFO, CALC Ins FIFO and SAVE Ins FIFO as shown in Fig. 10. The instructions in the FIFOs are issued when the corresponding hardware is free. Condition 2 is maintained by checking the dependency code in dep check module.


Michael Cashmore et al. Designed ROSPlan, a Planning in the Robot Operating System\cite{Cashmore2015}. In the general overview of the ROSPlan framework , consisting of the Knowledge Base and Planning System ROS nodes. Sensor data is passed continuously to ROSPlan, used to construct planning problem instances and inform the dispatch of the plan. Actions are dispatched as ROS actions and executed by lower-level controllers which respond reactively to immediate events and provide feedback Architectural overview of the ROSP LAN frame work. Blue boxes are ROS nodes. An example instantiation of the Knowledge Base is illustrated. Sensor data is interpreted for addition to the ontology, and actions are dispatched as ROS actions from the Planning System. The Planning System (PS) and Knowledge Base (KB) communicate during construction of the initial state and plan dispatch. During dispatch the PS will update the KB with the planning filter, and the KB may notify the PS of changes to the environment.

In Microsoft Project Brainwave\cite{Fowers2018}, Jeremy Fowers et al. describe the hardware architecture of a neural processing unit (NPU) designed as a production-scale system for real-time AI and tested on an Intel Stratix 10 280 \gls{fpga}. Real-time AI refers to interactive AI-driven services for low-latency evaluation of \gls{dnn}  models. The NPU architecture design for real-time AI services focuses on low latency, high throughput, and high efficiency of \gls{dnn} model execution. The NPU uses a single-threaded SIMD instruction set architecture (ISA) to achieve good performance in terms of latency and throughput. Brainwave's top-level scheduler must decode each instruction from a scalar control processor into thousands of primitive operations to control operations on many spatially distributed computing resources that scale up to 96,000 multiply-add units and function units for And the vector configuration control signal is based on the arbitration network of the BW ISA instruction chain it receives. The controller architecture is supported by a hierarchical instruction decoder and scheduler and thousands of independently addressable high-bandwidth on-chip memories, and can transparently exploit multiple levels of fine-grained SIMD parallelism. Hierarchical Decoding and Scheduling (HDD) logic extends compound operations into distributed control signals, managing thousands of compute units and dozens of register files and switches. The Brainwave top-level scheduler dispatches to 6 decoders and 4 second-level schedulers, which in turn dispatch to an additional 41 decoders. This scheme, combined with buffering at each stage, keeps the entire compute pipeline running, dispatching a compound instruction from Nios on average every four clock cycles.